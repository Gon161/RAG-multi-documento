# ğŸ“š RAG Multi-Documento - Chatbot Inteligente

Sistema de RecuperaciÃ³n de InformaciÃ³n Aumentada (RAG) que permite hacer preguntas sobre mÃºltiples documentos PDF simultÃ¡neamente, utilizando embeddings de OpenAI y ChromaDB como base de datos vectorial.

![Python](https://img.shields.io/badge/Python-3.10-blue)
![Flask](https://img.shields.io/badge/Flask-3.1.1-green)
![LangChain](https://img.shields.io/badge/LangChain-0.3.27-orange)
![OpenAI](https://img.shields.io/badge/OpenAI-GPT--4o--mini-purple)

## ğŸŒŸ Demo en Vivo

Prueba el sistema aquÃ­: **[https://rag-multi-documento.onrender.com/](https://rag-multi-documento.onrender.com/)**

## ğŸ¯ Â¿QuÃ© es este proyecto?

Un sistema RAG (Retrieval-Augmented Generation) que combina la bÃºsqueda semÃ¡ntica en documentos con la capacidad de generaciÃ³n de respuestas de modelos de lenguaje grandes (LLMs). Permite:

- ğŸ“„ Subir mÃºltiples documentos PDF
- ğŸ” Hacer preguntas sobre uno o varios documentos
- ğŸ“Š Obtener respuestas con referencias a las fuentes
- ğŸ¯ Filtrar bÃºsquedas por documentos especÃ­ficos
- ğŸ‘ï¸ Visualizar PDFs almacenados
- ğŸ§  Memoria conversacional que mantiene el contexto

## ğŸš€ CaracterÃ­sticas Principales

### âœ¨ Funcionalidades

- **Procesamiento de PDFs**: Extrae y segmenta texto de documentos PDF
- **Embeddings Vectoriales**: Utiliza OpenAI Embeddings para bÃºsqueda semÃ¡ntica
- **Base de Datos Vectorial**: ChromaDB para almacenamiento persistente
- **Chat Inteligente**: Respuestas contextuales usando GPT-4o-mini
- **Memoria Conversacional**: Recuerda el contexto de preguntas anteriores
- **BÃºsqueda Selectiva**: Consulta todos los documentos o solo los seleccionados
- **VisualizaciÃ³n de PDFs**: Visor integrado de documentos
- **GestiÃ³n de Documentos**: Sube, visualiza y elimina documentos fÃ¡cilmente

### ğŸ¨ Interfaz

- DiseÃ±o moderno con gradientes y efectos visuales
- Panel lateral para gestiÃ³n de documentos
- Ãrea de chat con formato de mensajes
- Indicadores de estado en tiempo real
- Notificaciones tipo toast
- Visualizador de PDF modal

## ğŸ› ï¸ TecnologÃ­as Utilizadas

### Backend
- **Flask**: Framework web Python
- **LangChain**: Framework para aplicaciones LLM
- **OpenAI**: API para embeddings y generaciÃ³n
- **ChromaDB**: Base de datos vectorial
- **PDFPlumber**: ExtracciÃ³n de texto de PDFs

### Frontend
- **HTML5/CSS3**: Interfaz moderna y responsiva
- **JavaScript Vanilla**: Sin dependencias de frameworks
- **Fetch API**: ComunicaciÃ³n asÃ­ncrona con el backend

## ğŸ“‹ Requisitos Previos

- Python 3.10.0
- Cuenta de OpenAI con API Key
- Navegador web moderno

## ğŸ”§ InstalaciÃ³n Local (Sin Entorno Virtual)

### 1. Clonar el Repositorio

```bash
git clone <tu-repositorio>
cd rag-multi-documento
```

### 2. Instalar Dependencias

```bash
pip install -r requirements.txt
```

### 3. Configurar Variables de Entorno

Crea un archivo `.env` en la raÃ­z del proyecto:

```env
OPENAI_API_KEY=tu_api_key_aqui
```

> âš ï¸ **Importante**: Nunca subas tu API key a GitHub. El archivo `.env` estÃ¡ incluido en `.gitignore`.

### 4. Ejecutar la AplicaciÃ³n

```bash
python app.py
```

La aplicaciÃ³n estarÃ¡ disponible en: `http://localhost:5000`

## ğŸŒ Despliegue en Render

Este proyecto estÃ¡ configurado para desplegarse en Render automÃ¡ticamente:

1. Conecta tu repositorio de GitHub con Render
2. Render detectarÃ¡ automÃ¡ticamente el archivo `render.yaml`
3. Configura la variable de entorno `OPENAI_API_KEY` en el dashboard de Render
4. El despliegue se iniciarÃ¡ automÃ¡ticamente

## ğŸ“– CÃ³mo Funciona

### Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Usuario   â”‚â”€â”€â”€â”€â–¶â”‚   Frontend   â”‚â”€â”€â”€â”€â–¶â”‚   Backend   â”‚
â”‚             â”‚â—€â”€â”€â”€â”€â”‚   (HTML/JS)  â”‚â—€â”€â”€â”€â”€â”‚   (Flask)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                â”‚
                                                â–¼
                                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                         â”‚   OpenAI    â”‚
                                         â”‚  Embeddings â”‚
                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                â”‚
                                                â–¼
                                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                         â”‚  ChromaDB   â”‚
                                         â”‚   Vectores  â”‚
                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flujo de Procesamiento

1. **Carga de Documento**:
   - Usuario sube un PDF
   - PDFPlumber extrae el texto
   - RecursiveCharacterTextSplitter divide en chunks (1000 chars)
   - OpenAI Embeddings genera vectores
   - ChromaDB almacena chunks con metadatos

2. **Consulta**:
   - Usuario hace una pregunta
   - Sistema genera embedding de la pregunta
   - ChromaDB busca chunks mÃ¡s relevantes (k=6)
   - ConversationalRetrievalChain genera respuesta
   - Se incluye contexto conversacional previo

3. **Respuesta Inteligente**:
   - Si hay info en documentos â†’ Respuesta con fuentes
   - Si no hay info â†’ Consulta directa al modelo GPT-4o-mini
   - Memoria conversacional mantiene contexto

## ğŸ“ Estructura del Proyecto

```
rag-multi-documento/
â”‚
â”œâ”€â”€ app.py                      # Backend Flask principal
â”œâ”€â”€ requirements.txt            # Dependencias Python
â”œâ”€â”€ render.yaml                # ConfiguraciÃ³n Render
â”œâ”€â”€ .env                       # Variables de entorno (NO SUBIR)
â”œâ”€â”€ .gitignore                 # Archivos ignorados por Git
â”œâ”€â”€ .python-version            # VersiÃ³n Python (3.10.0)
â”‚
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html             # Frontend Ãºnico
â”‚
â”œâ”€â”€ chroma_db_multi/           # Base de datos vectorial
â”œâ”€â”€ stored_pdfs/               # PDFs almacenados
â”œâ”€â”€ TextoEstraido/             # Textos extraÃ­dos de PDFs
â””â”€â”€ documents_metadata.json    # Metadatos de documentos
```

## ğŸ® Uso del Sistema

### 1. Subir Documentos

1. Haz clic en "ğŸ“ Seleccionar PDF"
2. Elige un archivo PDF
3. Presiona "ğŸš€ Procesar y Entrenar"
4. Espera la confirmaciÃ³n âœ…

### 2. Hacer Preguntas

1. Selecciona modo de bÃºsqueda:
   - ğŸŒ **Todos los documentos**: Busca en toda la biblioteca
   - ğŸ¯ **Documentos seleccionados**: Solo en los marcados

2. Escribe tu pregunta en el chat
3. Presiona "ğŸ“¤ Enviar" o Enter

### 3. Gestionar Documentos

- **Ver PDF**: Clic en ğŸ‘ï¸ para visualizar
- **Eliminar**: Clic en ğŸ—‘ï¸ para borrar
- **Seleccionar**: Checkbox para bÃºsquedas filtradas

### 4. Memoria Conversacional

- El sistema recuerda el contexto de la conversaciÃ³n
- Puedes hacer preguntas de seguimiento
- Usa "ğŸ§¹ Limpiar Memoria" para reiniciar el contexto

## ğŸ” Ejemplos de Uso

### Pregunta Simple
```
Usuario: "Â¿CuÃ¡les son los objetivos principales del proyecto?"
Bot: Responde basÃ¡ndose en los documentos + fuentes citadas
```

### Pregunta Conversacional
```
Usuario: "Â¿CuÃ¡l es el presupuesto del proyecto?"
Bot: "El presupuesto total es de $500,000..."

Usuario: "Â¿Y cuÃ¡nto se ha gastado hasta ahora?"
Bot: [Recuerda el contexto del presupuesto] "Se ha gastado $350,000..."
```

### Pregunta sin InformaciÃ³n
```
Usuario: "Â¿QuÃ© es la teorÃ­a de cuerdas?"
Bot: ğŸ“š No encontrÃ© informaciÃ³n especÃ­fica en los documentos...
     ğŸ¤– Sin embargo, puedo responder con mi conocimiento general: [...]
```

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Ajustar ParÃ¡metros RAG

En `app.py`, puedes modificar:

```python
# Modelo de chat
OPENAI_CHAT_MODEL = "gpt-4o-mini"  # O "gpt-4", "gpt-3.5-turbo"

# TamaÃ±o de chunks
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,      # Caracteres por chunk
    chunk_overlap=200     # Overlap entre chunks
)

# NÃºmero de chunks recuperados
def query_documents(question, k=6):  # k = nÃºmero de chunks
```

### Memoria Conversacional

```python
# Ajustar ventana de memoria
context = "\n".join([f"{msg.type}: {msg.content}" 
                     for msg in chat_history[-4:]])  # Ãšltimos 4 mensajes
```

## ğŸ› SoluciÃ³n de Problemas

### Error: "No se detectÃ³ API key"
- Verifica que el archivo `.env` existe
- Confirma que `OPENAI_API_KEY` estÃ¡ configurado
- Reinicia la aplicaciÃ³n

### Error al procesar PDF
- Verifica que el PDF no estÃ© protegido
- AsegÃºrate que el PDF contiene texto (no solo imÃ¡genes)
- Revisa los logs para mÃ¡s detalles

### Base de datos vectorial corrupta
```bash
# Eliminar y recrear
rm -rf chroma_db_multi/
python app.py  # Se recrea automÃ¡ticamente
```

## ğŸ“Š Rendimiento

- **Procesamiento**: ~5-10 segundos por PDF (depende del tamaÃ±o)
- **Respuestas**: ~2-5 segundos (depende de API OpenAI)
- **Capacidad**: Ilimitados documentos (limitado por disco/RAM)

## ğŸ” Seguridad

- âœ… Variables de entorno para API keys
- âœ… `.gitignore` configurado
- âœ… CORS habilitado para desarrollo
- âš ï¸ En producciÃ³n: configurar CORS especÃ­ficos
- âš ï¸ Implementar autenticaciÃ³n si es pÃºblico

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas:

1. Fork el proyecto
2. Crea tu rama (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“ Licencia

Este proyecto es de cÃ³digo abierto y estÃ¡ disponible bajo la licencia MIT.

## ğŸ‘¨â€ğŸ’» Autor

Desarrollado con â¤ï¸ para facilitar la bÃºsqueda y anÃ¡lisis de documentos mediante IA.

## ğŸ™ Agradecimientos

- [LangChain](https://langchain.com/) - Framework RAG
- [OpenAI](https://openai.com/) - Embeddings y LLM
- [ChromaDB](https://www.trychroma.com/) - Base de datos vectorial
- [Flask](https://flask.palletsprojects.com/) - Framework web

---

â­ Si te gusta este proyecto, dale una estrella en GitHub!